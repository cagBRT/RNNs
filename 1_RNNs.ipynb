{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/RNNs/blob/main/1_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -l -s https://github.com/cagBRT/RNNs.git cloned-repo\n",
        "%cd cloned-repo"
      ],
      "metadata": {
        "id": "8S739NoYKgSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "qlS62iYpKv5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXrIEr7a3OtE"
      },
      "source": [
        "This article explains RNNs - no code:\n",
        "\n",
        "https://machinelearningmastery.com/crash-course-recurrent-neural-networks-deep-learning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPX720YzP4Q"
      },
      "source": [
        "pre requisite:\n",
        "https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the architecture of the forward feed network.<br>\n",
        "An input layer, hidden layer or layers, and an output layer. The direction of information is always input to output. "
      ],
      "metadata": {
        "id": "ChkBSNnjLAO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"/content/cloned-repo/cloned-repo/images/forwardFeedArch.png\" , width=640)"
      ],
      "metadata": {
        "id": "FA3DtnvAKseN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK1JGgMFzTRj"
      },
      "source": [
        "A recurrent neural network (RNN) is a special type of an artificial neural network adapted to work for time series data or data that involves sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21fNVyy8zbLq"
      },
      "source": [
        "Data in a sequence such that one data point depends upon the previous data point, we need to modify the neural network to incorporate the dependencies between these data points. RNNs have the concept of ‘memory’ that helps them store the states or information of previous inputs to generate the next output of the sequence."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"/content/cloned-repo/cloned-repo/images/RNN.png\" , width=640)"
      ],
      "metadata": {
        "id": "d9RFClFTLU2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-E-j-iR0N2R"
      },
      "source": [
        "A simple RNN has a feedback loop as shown in the first diagram of the above figure. The feedback loop shown in the gray rectangle can be unrolled in 3 time steps to produce the second network of the above figure. Of course, you can vary the architecture so that the network unrolls  time steps. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijDphxqq0SOd"
      },
      "source": [
        "At every time step we can unfold the network for  time steps to get the output at each time step "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2t5duU90ajy"
      },
      "source": [
        "in the feedforward pass of a RNN, the network computes the values of the hidden units and the output after  time steps. The weights associated with the network are shared temporally. Each recurrent layer has two sets of weights; one for the input and the second one for the hidden unit. T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"/content/cloned-repo/images/RNNCloseUp.png\" , width=640)"
      ],
      "metadata": {
        "id": "N5Zq6krvLix3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFAF6bzL0oiX"
      },
      "source": [
        "The last feedforward layer, which computes the final output for the kth time step is just like an ordinary layer of a traditional feedforward network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NZEjDY20ykg"
      },
      "source": [
        "We can use any activation function we like in the recurrent neural network.<br>\n",
        "\n",
        "Common choices are:\n",
        "*   Sigmoid function\n",
        "*   Tanh function\n",
        "*   Relu function\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"/content/cloned-repo/images/activationFunctions.png\" , width=640)"
      ],
      "metadata": {
        "id": "dkJ5_XrjMaBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOkJx8gz0-bI"
      },
      "source": [
        "The backpropagation algorithm of an artificial neural network is modified to include the unfolding in time to train the weights of the network. This algorithm is based on computing the gradient vector and is called back propagation in time or BPTT algorithm for short"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brbjJGDg1CI5"
      },
      "source": [
        "Repeat till stopping criterion is met:<BR>\n",
        "Set all  to zero.<BR>\n",
        "Repeat for t = 0 to n-k<BR>\n",
        "1. Forward propagate the network over the unfolded network for  time steps to compute all  and .\n",
        "2. Compute the error as: \n",
        "3. Backpropagate the error across the unfolded network and update the weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRy_za9m1bO4"
      },
      "source": [
        "Types of RNNs<br>\n",
        ">One-to-one<br>\n",
        "One-to-many<br>\n",
        "Many-to-one<br>\n",
        "Many-to-many<br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"/content/cloned-repo/images/rnnsAchitectures.png\" , width=640)"
      ],
      "metadata": {
        "id": "90xnxEG0Nm9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsJhc8Ns1u7m"
      },
      "source": [
        "RNNs have various advantages such as:<br>\n",
        "\n",
        "* Ability to handle sequence data.<br>\n",
        "* Ability to handle inputs of varying lengths.<br>\n",
        "* Ability to store or ‘memorize’ <br>historical information.<br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyPBSluC13wM"
      },
      "source": [
        "The disadvantages are:<br>\n",
        "\n",
        "* The computation can be very slow.<br>\n",
        "* The network does not take into account future inputs to make decisions.<br>\n",
        "* Vanishing gradient problem, where the gradients used to compute the weight update may get very close to zero preventing the network from learning new weights. The deeper the network, the more pronounced is this problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9o6wi_y2VaH"
      },
      "source": [
        "**RNN Architectures**<br>\n",
        ">BRNN - Bidirectional RNN<br>\n",
        "GRU - Gated Recurrent Units<br>\n",
        "LSTM - Long Short Term Memory<br>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BWIQ-MV34Et"
      },
      "source": [
        "https://machinelearningmastery.com/understanding-simple-recurrent-neural-networks-in-keras/?utm_source=drip&utm_medium=email&utm_campaign=An+intro+to+recurrent+neural+networks&utm_content=An+intro+to+recurrent+neural+networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-B0OZeJyFgv"
      },
      "source": [
        "from pandas import read_csv\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T05pPKR4yeaw"
      },
      "source": [
        "def create_RNN(hidden_units, dense_units, input_shape, activation):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(hidden_units, input_shape=input_shape, \n",
        "                        activation=activation[0]))\n",
        "    model.add(Dense(units=dense_units, activation=activation[1]))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return model\n",
        " \n",
        "demo_model = create_RNN(2, 1, (3,1), activation=['linear', 'linear'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYoGHhAXylHM"
      },
      "source": [
        "wx = demo_model.get_weights()[0]\n",
        "wh = demo_model.get_weights()[1]\n",
        "bh = demo_model.get_weights()[2]\n",
        "wy = demo_model.get_weights()[3]\n",
        "by = demo_model.get_weights()[4]\n",
        " \n",
        "print('wx = ', wx, ' wh = ', wh, ' bh = ', bh, ' wy =', wy, 'by = ', by)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLyaSEP7yuES"
      },
      "source": [
        "x = np.array([1, 2, 3])\n",
        "# Reshape the input to the required sample_size x time_steps x features \n",
        "x_input = np.reshape(x,(1, 3, 1))\n",
        "y_pred_model = demo_model.predict(x_input)\n",
        " \n",
        " \n",
        "m = 2\n",
        "h0 = np.zeros(m)\n",
        "h1 = np.dot(x[0], wx) + h0 + bh\n",
        "h2 = np.dot(x[1], wx) + np.dot(h1,wh) + bh\n",
        "h3 = np.dot(x[2], wx) + np.dot(h2,wh) + bh\n",
        "o3 = np.dot(h3, wy) + by\n",
        " \n",
        "print('h1 = ', h1,'h2 = ', h2,'h3 = ', h3)\n",
        " \n",
        "print(\"Prediction from network \", y_pred_model)\n",
        "print(\"Prediction from our computation \", o3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ4B_bjM26QX"
      },
      "source": [
        "https://machinelearningmastery.com/global-attention-for-encoder-decoder-recurrent-neural-networks/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRlpqkdP27KX"
      },
      "source": [
        "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps6DL4B82943"
      },
      "source": [
        "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EHJGTfD3CWI"
      },
      "source": [
        "https://machinelearningmastery.com/crash-course-recurrent-neural-networks-deep-learning/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmQbCzVB3Jqw"
      },
      "source": [
        "Runningthe RNN on Sunspots Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lINF-Xmy4MAu"
      },
      "source": [
        "1. Read the dataset from a given URL\n",
        "2. Split the data into training and test set\n",
        "3. Prepare the input to the required Keras format\n",
        "4. Create an RNN model and train it\n",
        "5. Make the predictions on training and test sets and print the root mean square error on both sets\n",
        "6. View the result\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKCig1pq4bFq"
      },
      "source": [
        "# Parameter split_percent defines the ratio of training examples\n",
        "def get_train_test(url, split_percent=0.8):\n",
        "    df = read_csv(url, usecols=[1], engine='python')\n",
        "    data = np.array(df.values.astype('float32'))\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    data = scaler.fit_transform(data).flatten()\n",
        "    n = len(data)\n",
        "    # Point for splitting data into train and test\n",
        "    split = int(n*split_percent)\n",
        "    train_data = data[range(split)]\n",
        "    test_data = data[split:]\n",
        "    return train_data, test_data, data\n",
        " \n",
        "sunspots_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-sunspots.csv'\n",
        "train_data, test_data, data = get_train_test(sunspots_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1hd5kM94gQ5"
      },
      "source": [
        "# Prepare the input X and target Y\n",
        "def get_XY(dat, time_steps):\n",
        "    # Indices of target array\n",
        "    Y_ind = np.arange(time_steps, len(dat), time_steps)\n",
        "    Y = dat[Y_ind]\n",
        "    # Prepare X\n",
        "    rows_x = len(Y)\n",
        "    X = dat[range(time_steps*rows_x)]\n",
        "    X = np.reshape(X, (rows_x, time_steps, 1))    \n",
        "    return X, Y\n",
        " \n",
        "time_steps = 12\n",
        "trainX, trainY = get_XY(train_data, time_steps)\n",
        "testX, testY = get_XY(test_data, time_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWJYWC9e4lP8"
      },
      "source": [
        "model = create_RNN(hidden_units=3, dense_units=1, input_shape=(time_steps,1), \n",
        "                   activation=['tanh', 'tanh'])\n",
        "model.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLTED1bu4qsW"
      },
      "source": [
        "def print_error(trainY, testY, train_predict, test_predict):    \n",
        "    # Error of predictions\n",
        "    train_rmse = math.sqrt(mean_squared_error(trainY, train_predict))\n",
        "    test_rmse = math.sqrt(mean_squared_error(testY, test_predict))\n",
        "    # Print RMSE\n",
        "    print('Train RMSE: %.3f RMSE' % (train_rmse))\n",
        "    print('Test RMSE: %.3f RMSE' % (test_rmse))    \n",
        " \n",
        "# make predictions\n",
        "train_predict = model.predict(trainX)\n",
        "test_predict = model.predict(testX)\n",
        "# Mean square error\n",
        "print_error(trainY, testY, train_predict, test_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT_Y4kK54xL0"
      },
      "source": [
        "def plot_result(trainY, testY, train_predict, test_predict):\n",
        "    actual = np.append(trainY, testY)\n",
        "    predictions = np.append(train_predict, test_predict)\n",
        "    rows = len(actual)\n",
        "    plt.figure(figsize=(15, 6), dpi=80)\n",
        "    plt.plot(range(rows), actual)\n",
        "    plt.plot(range(rows), predictions)\n",
        "    plt.axvline(x=len(trainY), color='r')\n",
        "    plt.legend(['Actual', 'Predictions'])\n",
        "    plt.xlabel('Observation number after given time steps')\n",
        "    plt.ylabel('Sunspots scaled')\n",
        "    plt.title('Actual and Predicted Values. The Red Line Separates The Training And Test Examples')\n",
        "plot_result(trainY, testY, train_predict, test_predict)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}